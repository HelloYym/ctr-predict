{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import time\n",
    "import pickle\n",
    "import gc\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import load_pickle, dump_pickle, get_feature_value, feature_spearmanr, feature_target_spearmanr, addCrossFeature, calibration\n",
    "from utils import raw_data_path, feature_data_path, cache_pkl_path, analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "230"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_path = feature_data_path + 'all_data_all_features.pkl'\n",
    "all_data = load_pickle(all_data_path)\n",
    "\n",
    "target = 'is_trade'\n",
    "\n",
    "features = load_pickle(feature_data_path + 'features_0418_fewer.pkl')\n",
    "\n",
    "len(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用18-23号数据训练 lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user02/miniconda3/envs/gluon/lib/python3.6/site-packages/lightgbm/basic.py:1038: UserWarning: categorical_feature in Dataset is overrided. New categorical_feature is ['user_click_rank_day', 'user_gender_id', 'user_occupation_id']\n",
      "  warnings.warn('categorical_feature in Dataset is overrided. New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.08110331184334206, 0.079060620253194033)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "train_data = all_data[(all_data.day >= 20) & (all_data.day <= 23)]\n",
    "test_data = all_data[all_data.day == 24]\n",
    "\n",
    "lgb_clf = LGBMClassifier(n_estimators=200, max_depth=3, )\n",
    "\n",
    "cate_features = ['user_gender_id', 'user_occupation_id', 'user_click_rank_day']\n",
    "\n",
    "lgb_clf.fit(train_data[features], train_data['is_trade'],\n",
    "            feature_name=features,\n",
    "            categorical_feature=cate_features,\n",
    "            verbose=50,\n",
    "            )\n",
    "\n",
    "loss_train = log_loss(train_data[target],lgb_clf.predict_proba(train_data[features]))\n",
    "loss_test = log_loss(test_data[target], lgb_clf.predict_proba(test_data[features]))\n",
    "\n",
    "loss_train, loss_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(496482, 1788)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_leaves = lgb_clf.apply(all_data[features])\n",
    "\n",
    "lgb_leaves = pd.DataFrame(lgb_leaves).astype(np.int32)\n",
    "\n",
    "# 转换为one-hot\n",
    "lgb_leaves = pd.get_dummies(lgb_leaves, dummy_na=True, columns=lgb_leaves.columns)\n",
    "lgb_leaves.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用18-23号数据训练 xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.082443637414406287, 0.07825402801893741)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "import xgboost as xgb\n",
    "\n",
    "train_data = all_data[(all_data.day >= 19) & (all_data.day <= 23)]\n",
    "test_data = all_data[all_data.day == 24]\n",
    "\n",
    "xgb_clf = xgb.XGBClassifier(objective='binary:logistic',\n",
    "\n",
    "                             n_estimators=200,\n",
    "                             learning_rate=0.1,\n",
    "\n",
    "                             max_depth=3,\n",
    "                             min_child_weight=1e-3,\n",
    "                             gamma=0,\n",
    "\n",
    "                             colsample_bytree=0.8,\n",
    "                             subsample=0.9,\n",
    "\n",
    "                             reg_lambda=10,\n",
    "                             min_split_gain=0.,\n",
    "                             \n",
    "                             max_bin=63,\n",
    "\n",
    "                             n_jobs=6,\n",
    "                             silent=False,\n",
    "                             )\n",
    "\n",
    "\n",
    "xgb_clf.fit(train_data[features], train_data[target],\n",
    "#             eval_set=[(test_data[features], test_data[target])],\n",
    "#             early_stopping_rounds=50,\n",
    "#             eval_metric='logloss',\n",
    "#             verbose=10,\n",
    "            )\n",
    "\n",
    "loss_train = log_loss(train_data[target], xgb_clf.predict_proba(train_data[features]))\n",
    "loss_test = log_loss(test_data[target], xgb_clf.predict_proba(test_data[features]))\n",
    "\n",
    "loss_train, loss_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(539370, 1794)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_leaves = xgb_clf.apply(all_data[features])\n",
    "\n",
    "xgb_leaves = pd.DataFrame(xgb_leaves).astype(np.int32)\n",
    "\n",
    "# 转换为one-hot\n",
    "xgb_leaves = pd.get_dummies(xgb_leaves, dummy_na=True, columns=xgb_leaves.columns)\n",
    "xgb_leaves.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nominal_feats = [\n",
    "\n",
    "    'user_gender_id',\n",
    "    'user_occupation_id',\n",
    "    'context_page_id',\n",
    "    'user_last_click_day',\n",
    "    'category2_label',\n",
    "    'category_predict_rank',\n",
    "\n",
    "#     'user_item_id_click_rank',\n",
    "#     'user_item_brand_id_click_rank',\n",
    "#     'user_shop_id_click_rank',\n",
    "#     'user_context_page_id_click_rank',\n",
    "#     'user_category2_label_click_rank',\n",
    "\n",
    "#     'user_item_id_click_rank_day',\n",
    "#     'user_item_brand_id_click_rank_day',\n",
    "#     'user_shop_id_click_rank_day',\n",
    "#     'user_context_page_id_click_rank_day',\n",
    "#     'user_category2_label_click_rank_day',\n",
    "]\n",
    "\n",
    "# nominal_feats = ['user_gender_id', 'user_occupation_id', 'context_page_id']\n",
    "\n",
    "# numeric_feats = ['item_price_level', 'item_sales_level', 'item_collected_level', 'item_pv_level', 'user_age_level', 'user_star_level',\n",
    "#                  'shop_review_num_level', 'shop_review_positive_rate', 'shop_star_level', 'shop_score_service', 'shop_score_delivery', 'shop_score_description']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 原始特征与xgb叶节点特征合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# xgb叶节点+原始特征\n",
    "# all_data_with_leaves = pd.concat([all_data, xgb_leaves], axis=1)\n",
    "\n",
    "# 只用xgb叶节点特征\n",
    "all_data_with_leaves = pd.concat(\n",
    "    [all_data[['day', 'is_trade', 'instance_id'] + nominal_feats], xgb_leaves], axis=1)\n",
    "\n",
    "# 标称属性转换为one-hot\n",
    "all_data_with_leaves = pd.get_dummies(\n",
    "    all_data_with_leaves, dummy_na=True, columns=nominal_feats)\n",
    "\n",
    "features = list(all_data_with_leaves.columns)\n",
    "features.remove('is_trade')\n",
    "features.remove('instance_id')\n",
    "features.remove('day')\n",
    "target = 'is_trade'\n",
    "len(features)\n",
    "\n",
    "\n",
    "# 数值特征归一化\n",
    "all_data_with_leaves[features] = all_data_with_leaves[features].apply(\n",
    "    lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    "\n",
    "all_data_with_leaves = all_data_with_leaves.fillna(-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据转换为 mxnet.ndarray 格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(420693, 944)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = all_data_with_leaves[(all_data_with_leaves.day >= 18) & (all_data_with_leaves.day <= 23)]\n",
    "test_data = all_data_with_leaves[all_data_with_leaves.day == 24]\n",
    "\n",
    "from mxnet import ndarray as nd\n",
    "from mxnet import autograd\n",
    "from mxnet import gluon\n",
    "import mxnet as mx\n",
    "\n",
    "X_train = train_data[features].as_matrix()\n",
    "X_test = test_data[features].as_matrix()\n",
    "y_train = train_data[target].astype(np.int).as_matrix()\n",
    "y_test = test_data[target].astype(np.int).as_matrix()\n",
    "\n",
    "X_train = nd.array(X_train)\n",
    "X_test = nd.array(X_test)\n",
    "\n",
    "y_train = nd.array(y_train).reshape((-1, 1))\n",
    "y_test = nd.array(y_test).reshape((-1, 1))\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义网络结构和loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ctx = mx.gpu(0)\n",
    "# ctx = mx.cpu(0)\n",
    "\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "\n",
    "def get_lr():\n",
    "    net = nn.Sequential()\n",
    "    with net.name_scope():\n",
    "#         net.add(nn.Dense(64, activation=\"relu\"))\n",
    "        net.add(nn.Dense(2))\n",
    "    net.initialize(ctx=ctx)\n",
    "    return net\n",
    "\n",
    "\n",
    "def get_net_dropout(drop_prob1, drop_prob2):\n",
    "    net = gluon.nn.Sequential()\n",
    "\n",
    "    with net.name_scope():\n",
    "        # 第一层全连接。\n",
    "        net.add(nn.Dense(32, activation=\"relu\"))\n",
    "        # 在第一层全连接后添加丢弃层。\n",
    "        net.add(nn.Dropout(drop_prob1))\n",
    "#         # 第二层全连接。\n",
    "        net.add(nn.Dense(32, activation=\"relu\"))\n",
    "        # 在第二层全连接后添加丢弃层。\n",
    "        net.add(nn.Dropout(drop_prob2))\n",
    "        net.add(nn.Dense(2))\n",
    "    net.initialize(ctx=ctx)\n",
    "    return net\n",
    "\n",
    "\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "\n",
    "def evaluate_loss(net, data_iter):\n",
    "    total_loss = 0.\n",
    "    n = 0\n",
    "    for data, label in data_iter:\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        output = net(data)\n",
    "        loss = softmax_cross_entropy(output, label)\n",
    "        total_loss += nd.sum(loss).asscalar()\n",
    "        n += label.size\n",
    "    return total_loss / n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi'] = 120\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "import utils\n",
    "\n",
    "def train(net, X_train, y_train, X_test, y_test, epochs,\n",
    "          verbose_epoch, batch_size, learning_rate, weight_decay, lr_decay, lr_decay_epoch):\n",
    "    \"\"\"Train a network\"\"\"\n",
    "    print(\"Start training on \", ctx)\n",
    "    \n",
    "    train_loss = []\n",
    "\n",
    "    dataset_train = gluon.data.ArrayDataset(X_train, y_train)\n",
    "    data_iter_train = utils.DataLoader(\n",
    "        dataset_train, batch_size, shuffle=True)\n",
    "\n",
    "    if X_test is not None:\n",
    "        test_loss = []\n",
    "        dataset_test = gluon.data.ArrayDataset(X_test, y_test)\n",
    "        data_iter_test = gluon.data.DataLoader(\n",
    "            dataset_test, batch_size, shuffle=False)\n",
    "\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'adam',\n",
    "                            {'learning_rate': learning_rate, 'wd': weight_decay})\n",
    "    \n",
    "    net.collect_params().initialize(force_reinit=True, ctx=ctx)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start = time()\n",
    "        for data, label in data_iter_train:\n",
    "            with autograd.record():\n",
    "                data = data.as_in_context(ctx)\n",
    "                label = label.as_in_context(ctx)\n",
    "                output = net(data)\n",
    "                loss = softmax_cross_entropy(output, label)\n",
    "\n",
    "            loss.backward()\n",
    "            trainer.step(batch_size)\n",
    "            nd.waitall()\n",
    "\n",
    "#         if epoch > 0 and epoch % lr_decay_epoch == 0:\n",
    "#             trainer.set_learning_rate(trainer.learning_rate * lr_decay)\n",
    "#             print('change lr to %f' % (trainer.learning_rate))\n",
    "\n",
    "        if epoch >= verbose_epoch:\n",
    "            cur_train_loss = evaluate_loss(net, data_iter_train)\n",
    "            train_loss.append(cur_train_loss)\n",
    "\n",
    "            if X_test is not None:\n",
    "                cur_test_loss = evaluate_loss(net, data_iter_test)\n",
    "                test_loss.append(cur_test_loss)\n",
    "\n",
    "            if X_test is not None:\n",
    "                print(\"Epoch %d, train loss: %f, test loss: %f, Time %.1f sec\" % (\n",
    "                    epoch, cur_train_loss, cur_test_loss, time() - start))\n",
    "            else:\n",
    "                print(\"Epoch %d, train loss: %f, Time %.1f sec\" %\n",
    "                      (epoch, cur_train_loss, time() - start))\n",
    "\n",
    "    plt.plot(train_loss)\n",
    "    plt.legend(['train'])\n",
    "    if X_test is not None:\n",
    "        plt.plot(test_loss)\n",
    "        plt.legend(['train', 'test'])\n",
    "    plt.show()\n",
    "    if X_test is not None:\n",
    "        return cur_train_loss, cur_test_loss\n",
    "    else:\n",
    "        return cur_train_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "verbose_epoch = 0\n",
    "learning_rate = 0.005\n",
    "batch_size = 10000\n",
    "lr_decay = 0.2\n",
    "lr_decay_epoch = 30\n",
    "weight_decay = 0.1\n",
    "\n",
    "\n",
    "drop_prob1 = 0.2\n",
    "drop_prob2 = 0.2\n",
    "\n",
    "# net = get_net_dropout(drop_prob1, drop_prob2)\n",
    "net = get_lr()\n",
    "\n",
    "train_loss, test_loss = train(net, X_train, y_train, X_test, y_test, epochs, verbose_epoch, batch_size, learning_rate, weight_decay, lr_decay, lr_decay_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    exp = nd.exp(X)\n",
    "    # 假设exp是矩阵，这里对行进行求和，并要求保留axis 1，\n",
    "    # 就是返回 (nrows, 1) 形状的矩阵\n",
    "    partition = exp.sum(axis=1, keepdims=True)\n",
    "    return exp / partition\n",
    "\n",
    "train_predict = softmax(net(X_train.as_in_context(ctx)))[:,1].as_in_context(mx.cpu()).asnumpy()\n",
    "test_predict = softmax(net(X_test.as_in_context(ctx)))[:,1].as_in_context(mx.cpu()).asnumpy()\n",
    "\n",
    "train_predict, test_predict\n",
    "\n",
    "loss_train = log_loss(train_data[target], train_predict)\n",
    "loss_test = log_loss(test_data[target], test_predict)\n",
    "\n",
    "loss_train, loss_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
