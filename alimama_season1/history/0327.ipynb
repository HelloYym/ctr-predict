{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 时间戳转字符串\n",
    "def timestamp_datetime(value):\n",
    "    format = '%Y-%m-%d %H:%M:%S'\n",
    "    value = time.localtime(value)\n",
    "    dt = time.strftime(format, value)\n",
    "    return dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 提取特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numeric_feats = ['item_price_level', 'item_sales_level', 'item_collected_level', 'item_pv_level', 'user_age_level',\n",
    "                 'user_star_level', 'shop_review_num_level',\n",
    "                 'shop_review_positive_rate', 'shop_star_level', 'shop_score_service', 'shop_score_delivery', 'shop_score_description',\n",
    "                 'hour', 'day',\n",
    "                 'user_query_day', 'user_query_day_hour',\n",
    "                 'item_CTR', 'shop_CTR', \n",
    "                 ]\n",
    "\n",
    "nominal_feats = ['item_id', 'user_id', 'shop_id', 'item_brand_id',\n",
    "                 'item_city_id', 'user_gender_id', 'user_occupation_id', 'context_page_id']\n",
    "\n",
    "features = numeric_feats + nominal_feats\n",
    "\n",
    "target = 'is_trade'\n",
    "\n",
    "def extract_date(data):\n",
    "    data['time'] = data.context_timestamp.apply(timestamp_datetime)\n",
    "    data['day'] = data.time.apply(lambda x: int(x[8:10]))\n",
    "    data['hour'] = data.time.apply(lambda x: int(x[11:13]))\n",
    "    data['minute'] = data.time.apply(lambda x: int(x[14:15]))\n",
    "    del data['time']\n",
    "    return data\n",
    "\n",
    "def extract_ctr(data, feature, alias):\n",
    "    '''统计给定数据的转化率\n",
    "    \n",
    "    '''\n",
    "    ctr_feat = alias + '_CTR'\n",
    "    query_cnt_feat = alias + '_query_cnt_history'\n",
    "    conversion_cnt_feat = alias + '_conversion_cnt_history'\n",
    "    \n",
    "    \n",
    "    query_cnt = data.groupby([feature]).size().reset_index().rename(columns={0: query_cnt_feat})\n",
    "    conversion_cnt = data[data['is_trade'] == 1].groupby([feature]).size().reset_index().rename(columns={0: conversion_cnt_feat})\n",
    "    ctr = pd.merge(query_cnt, conversion_cnt, how='left', on=[feature])\n",
    "    ctr[[conversion_cnt_feat]] = ctr[[conversion_cnt_feat]].fillna(0)\n",
    "    ctr[ctr_feat] = ctr[conversion_cnt_feat] / ctr[query_cnt_feat]\n",
    "    return ctr\n",
    "\n",
    "def extract_history_ctr(data, feature, alias):\n",
    "    '''统计每一天之前的历史转化率\n",
    "    \n",
    "    '''\n",
    "    history_ctr = pd.DataFrame()\n",
    "    \n",
    "    for day in range(18, 26):\n",
    "        # 每一天之前的历史数据\n",
    "        history_data = data[data['day'] < day]\n",
    "        ctr = extract_ctr(history_data, feature, alias)        \n",
    "    \n",
    "        # 添加date字段，方便merge\n",
    "        ctr['day'] = day\n",
    "        history_ctr = history_ctr.append(ctr)\n",
    "        \n",
    "    return history_ctr\n",
    "\n",
    "def extract_last_day_ctr(data):\n",
    "    '''统计前一天的总体转化率\n",
    "    \n",
    "    '''\n",
    "    days_ctr = pd.DataFrame()\n",
    "    ctr_feat = 'last_day_CTR'\n",
    "    query_cnt_feat = 'last_day_query_cnt'\n",
    "    conversion_cnt_feat = 'last_day_conversion_cnt'\n",
    "    \n",
    "    for day in range(19, 26):\n",
    "        # 每一天之前的历史数据\n",
    "        history_data = data[data['day'] == day - 1]\n",
    "\n",
    "        query_cnt = history_data.shape[0]\n",
    "        conversion_cnt = history_data[history_data['is_trade'] == 1].shape[0]\n",
    "        ctr = conversion_cnt / query_cnt \n",
    "    \n",
    "        # 添加date字段，方便merge\n",
    "        days_ctr = days_ctr.append({ctr_feat: ctr, query_cnt_feat:query_cnt, conversion_cnt_feat:conversion_cnt, 'day':day}, ignore_index=True)\n",
    "        \n",
    "    return days_ctr\n",
    "\n",
    "def extract_user_item_conversion(data):\n",
    "    '''统计已经转化的user-item组合\n",
    "    \n",
    "    '''\n",
    "    history_user_item_conversion = pd.DataFrame()\n",
    "    \n",
    "    for day in range(19, 26):\n",
    "        # 每一天之前的历史数据\n",
    "        user_item_conversion = data[(data['day'] < day)][['user_id', 'item_id']]\n",
    "        user_item_conversion['converted'] = 1\n",
    "        user_item_conversion['day'] = day\n",
    "        user_item_conversion.drop_duplicates(inplace=True)\n",
    "        \n",
    "        # 添加date字段，方便merge\n",
    "        history_user_item_conversion = history_user_item_conversion.append(user_item_conversion, ignore_index=True)\n",
    "        \n",
    "    return history_user_item_conversion\n",
    "\n",
    "def extract_user_item_click(data):\n",
    "    '''统计历史的user-item组合次数\n",
    "    \n",
    "    '''\n",
    "    history_user_item_click = pd.DataFrame()\n",
    "    feat = 'user_item_click'\n",
    "    \n",
    "    for day in range(19, 26):\n",
    "        # 每一天之前的历史数据\n",
    "        user_item_click = data[(data['day'] < day)][['user_id', 'item_id']]\n",
    "        user_item_click_cnt = user_item_click.groupby(['user_id', 'item_id']).size().reset_index().rename(columns={0: 'user_item_click_cnt'})\n",
    "        user_item_click_cnt['day'] = day\n",
    "        \n",
    "        # 添加date字段，方便merge\n",
    "        history_user_item_click = history_user_item_click.append(user_item_click_cnt, ignore_index=True)\n",
    "        \n",
    "    return history_user_item_click\n",
    "    \n",
    "\n",
    "def extract_manual_features(data):\n",
    "    \n",
    "    user_item_query_day = data.groupby(['user_id', 'item_id', 'day']).size().reset_index().rename(columns={0: 'user_item_query_day'})\n",
    "    data = pd.merge(data, user_item_query_day, how='left',on=['user_id', 'item_id', 'day'])\n",
    "    \n",
    "    user_item_query_day_hour = data.groupby(['user_id', 'item_id', 'day', 'hour']).size().reset_index().rename(columns={0: 'user_item_query_day_hour'})\n",
    "    data = pd.merge(data, user_item_query_day_hour, how='left',on=['user_id', 'item_id', 'day', 'hour'])\n",
    "\n",
    "        \n",
    "#     ###############\n",
    "    \n",
    "    user_query_day = data.groupby(['user_id', 'day']).size().reset_index().rename(columns={0: 'user_query_day'})\n",
    "    data = pd.merge(data, user_query_day, how='left',on=['user_id', 'day'])\n",
    "\n",
    "    user_query_day_hour = data.groupby(['user_id', 'day', 'hour']).size().reset_index().rename(columns={0: 'user_query_day_hour'})\n",
    "    data = pd.merge(data, user_query_day_hour, how='left',on=['user_id', 'day', 'hour'])\n",
    "    \n",
    "    item_query_day = data.groupby(['item_id', 'day']).size().reset_index().rename(columns={0: 'item_query_day'})\n",
    "    data = pd.merge(data, item_query_day, 'left', on=['item_id', 'day'])\n",
    "    \n",
    "    item_query_day_hour = data.groupby(['item_id', 'day', 'hour']).size().reset_index().rename(columns={0: 'item_query_day_hour'})\n",
    "    data = pd.merge(data, item_query_day_hour, 'left',on=['item_id', 'day', 'hour'])\n",
    "    \n",
    "    shop_query_day = data.groupby(['shop_id', 'day']).size().reset_index().rename(columns={0: 'shop_query_day'})\n",
    "    data = pd.merge(data, shop_query_day, 'left', on=['shop_id', 'day'])\n",
    "    \n",
    "    shop_query_day_hour = data.groupby(['shop_id', 'day', 'hour']).size().reset_index().rename(columns={0: 'shop_query_day_hour'})\n",
    "    data = pd.merge(data, shop_query_day_hour, 'left',on=['shop_id', 'day', 'hour'])\n",
    "    \n",
    "    \n",
    "#     ======= brand =======\n",
    "#     item_in_brand = data.groupby(['item_id', 'item_brand_id']).size().reset_index().rename(columns={0: 'item_in_brand'})\n",
    "#     data = pd.merge(data, user_brand_query_day, how='left',on=['item_id', 'item_brand_id'])\n",
    "\n",
    "    user_brand_query_day = data.groupby(['user_id', 'item_brand_id', 'day']).size().reset_index().rename(columns={0: 'user_brand_query_day'})\n",
    "    data = pd.merge(data, user_brand_query_day, how='left',on=['user_id', 'item_brand_id', 'day'])\n",
    "    \n",
    "    user_brand_query_day_hour = data.groupby(['user_id', 'item_brand_id', 'day', 'hour']).size().reset_index().rename(columns={0: 'user_brand_query_day_hour'})\n",
    "    data = pd.merge(data, user_brand_query_day_hour, how='left',on=['user_id', 'item_brand_id', 'day', 'hour'])\n",
    "    \n",
    "    brand_query_day = data.groupby(['item_brand_id', 'day']).size().reset_index().rename(columns={0: 'brand_query_day'})\n",
    "    data = pd.merge(data, brand_query_day, 'left', on=['item_brand_id', 'day'])\n",
    "    \n",
    "    brand_query_day_hour = data.groupby(['item_brand_id', 'day', 'hour']).size().reset_index().rename(columns={0: 'brand_query_day_hour'})\n",
    "    data = pd.merge(data, brand_query_day_hour, 'left',on=['item_brand_id', 'day', 'hour'])\n",
    "       \n",
    "    data['shop_to_user'] = data.user_brand_query_day / data.brand_query_day\n",
    "    \n",
    "        \n",
    "    history_item_ctr = extract_history_ctr(data, 'item_id', 'item')\n",
    "    data = pd.merge(data, history_item_ctr, how='left', on=['item_id', 'day'])\n",
    "    data[['item_CTR']] = data[['item_CTR']].fillna(-1)\n",
    "        \n",
    "    history_shop_ctr = extract_history_ctr(data, 'shop_id', 'shop')\n",
    "    data = pd.merge(data, history_shop_ctr, how='left', on=['shop_id', 'day'])\n",
    "    data[['shop_CTR']] = data[['shop_CTR']].fillna(-1)\n",
    "    \n",
    "    history_user_ctr = extract_history_ctr(data, 'user_id', 'user')\n",
    "    data = pd.merge(data, history_user_ctr, how='left', on=['user_id', 'day'])\n",
    "    data[['user_CTR']] = data[['user_CTR']].fillna(-1)\n",
    "    \n",
    "    history_day_ctr = extract_last_day_ctr(data)\n",
    "    data = pd.merge(data, history_day_ctr, how='left', on=['day'])\n",
    "    data[['last_day_CTR']] = data[['last_day_CTR']].fillna(-1)\n",
    "    \n",
    "    history_user_item_conversion = extract_user_item_conversion(data)\n",
    "    data = pd.merge(data, history_user_item_conversion, how='left', on=['user_id', 'item_id', 'day'])\n",
    "    data[['converted']] = data[['converted']].fillna(0)\n",
    "    \n",
    "#     history_user_item_click = extract_user_item_click(data)\n",
    "#     data = pd.merge(data, history_user_item_click, how='left', on=['user_id', 'item_id', 'day'])\n",
    "#     data[['user_item_click_cnt']] = data[['user_item_click_cnt']].fillna(0)\n",
    "\n",
    "    user_item_std = data.groupby(['user_id', 'item_id', 'day']).hour.std().reset_index().rename(columns={'hour': 'user_item_std'})\n",
    "    data = pd.merge(data, user_item_std, how='left', on=['user_id','item_id', 'day'])\n",
    "    \n",
    "    \n",
    "    feature_with_CTR = ['shop_id', 'item_id', 'day', 'user_id']\n",
    "    data[feature_with_CTR] = data[feature_with_CTR].astype(np.int64)\n",
    "    \n",
    "#     data = pd.get_dummies(data, dummy_na=True, columns=['user_gender_id', 'user_occupation_id', 'context_page_id'])\n",
    "\n",
    "    \n",
    "#     data = data.replace(to_replace=[-1], value=np.NaN)\n",
    "#     data[numeric_feats] = data[numeric_feats].fillna(data.mean())\n",
    "#     data[numeric_feats] = data[numeric_feats].apply(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取训练样本并转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"round1_ijcai_18_train_20180301.txt\", sep=' ')\n",
    "df_test = pd.read_csv(\"round1_ijcai_18_test_a_20180301.txt\", sep=' ')\n",
    "\n",
    "df_train.drop_duplicates(inplace=True)\n",
    "df_train = extract_date(df_train)\n",
    "df_test = extract_date(df_test)\n",
    "\n",
    "num_train = df_train.shape[0]\n",
    "all_data = pd.concat([df_train, df_test])\n",
    "all_data = extract_manual_features(all_data)\n",
    "df_train = all_data[:num_train]\n",
    "df_test = all_data[num_train:]\n",
    "del df_test['is_trade']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((420693, 57), (57418, 57))"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 根据日期，取24号为验证集\n",
    "train_data = df_train.loc[df_train.day < df_train['day'].max()]\n",
    "val_data = df_train.loc[df_train.day == df_train['day'].max()]\n",
    "\n",
    "train_data.shape, val_data.shape\n",
    "\n",
    "# train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_item_std = df_train.groupby(['user_id', 'user_item_query_day']).context_timestamp.count().reset_index().rename(columns={'context_timestamp': 'user_item_query_day_m'})\n",
    "\n",
    "# user_item_std[user_item_std['user_id'] == 228793539864019462]\n",
    "\n",
    "a = df_train[['item_brand_id']]\n",
    "\n",
    "# a.groupby(['item_brand_id'], as_index=False).count().rename(columns={0: 'item_brand_query_cnt_feat'})\n",
    "# df_train.groupby(['item_brand_id'], as_index=False).size().reset_index().rename(columns={0: 'item_brand_query_cnt_feat'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.018811531213462983"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract_ctr(df_train, feature='day', alias='page')\n",
    "extract_last_day_ctr(df_train)\n",
    "history_user_item_conversion = extract_user_item_conversion(df_train)\n",
    "# history_user_item_conversion[history_user_item_conversion['day']==23]\n",
    "\n",
    "a = df_train[(df_train['converted'] == 1) & (df_train['day'] == 23)][['user_id','item_id','is_trade','converted']]\n",
    "a[a['is_trade'] == 1].shape[0] / a.shape[0]\n",
    "\n",
    "# df_test[(df_test['converted'] == 1) & (df_test['day'] == 25)][['user_id','item_id','converted']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = ['item_price_level', 'item_sales_level', 'item_collected_level', 'item_pv_level', 'item_city_id', 'item_brand_id',\n",
    "            'user_age_level', 'user_star_level', 'user_gender_id', 'user_occupation_id',\n",
    "            'shop_review_num_level', 'shop_review_positive_rate', 'shop_star_level', 'shop_score_service',\n",
    "            'shop_score_delivery', 'shop_score_description', 'context_page_id',\n",
    "            'item_id', 'user_id', 'shop_id',\n",
    "            'day', 'hour',\n",
    "            'user_query_day',\n",
    "            'user_query_day_hour',\n",
    "            'item_query_day',\n",
    "            'item_query_day_hour',\n",
    "            'shop_query_day',\n",
    "            'shop_query_day_hour',\n",
    "            'item_CTR',\n",
    "            'shop_CTR',\n",
    "            'user_CTR',\n",
    "            \n",
    "#             'user_item_click_cnt',\n",
    "            \n",
    "            'item_conversion_cnt_history', \n",
    "            'user_conversion_cnt_history',\n",
    "            \n",
    "            'user_item_query_day',\n",
    "#             'user_item_query_day_hour',\n",
    "            \n",
    "            'user_brand_query_day_hour',\n",
    "            'user_brand_query_day',\n",
    "            \n",
    "#             'user_item_std',\n",
    "            \n",
    "#             'brand_query_day',\n",
    "#             'brand_query_day_hour',\n",
    "            'shop_to_user',\n",
    "            \n",
    "            \n",
    "            'last_day_CTR',\n",
    "            'last_day_conversion_cnt',\n",
    "            'last_day_query_cnt',\n",
    "            \n",
    "            'converted'\n",
    "            ]\n",
    "\n",
    "nominal_feats = ['user_gender_id', ]\n",
    "\n",
    "\n",
    "target = 'is_trade'\n",
    "\n",
    "# train_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user02/miniconda3/envs/gluon/lib/python3.6/site-packages/lightgbm/basic.py:1038: UserWarning: categorical_feature in Dataset is overrided. New categorical_feature is ['user_gender_id']\n",
      "  warnings.warn('categorical_feature in Dataset is overrided. New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.087448212455262769, 0.080831573469185056)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import make_scorer, precision_score, recall_score, accuracy_score, log_loss\n",
    "clf = lgb.LGBMClassifier(max_depth=3, n_estimators=200, n_jobs=-1)\n",
    "\n",
    "clf.fit(train_data[features], train_data[target], feature_name=features, categorical_feature=nominal_feats)\n",
    "\n",
    "loss_train = log_loss(train_data[target], clf.predict_proba(train_data[features]))\n",
    "\n",
    "predict_proba = clf.predict_proba(val_data[features])\n",
    "    \n",
    "loss_val = log_loss(val_data[target], predict_proba)\n",
    "\n",
    "loss_train, loss_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user02/miniconda3/envs/gluon/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.080831573469185056"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clf = lgb.LGBMClassifier(max_depth=3, n_estimators=200, n_jobs=-1)\n",
    "# clf.fit(df_train[features], df_train[target], feature_name=features, categorical_feature=['user_gender_id'])\n",
    "\n",
    "loss_train = log_loss(val_data[target], clf.predict_proba(val_data[features]))\n",
    "\n",
    "df_test['predicted_score'] = clf.predict_proba(df_test[features])[:, 1]\n",
    "\n",
    "df_test[['instance_id', 'predicted_score']].to_csv(\n",
    "    '20180328.txt', index=False, sep=' ')\n",
    "\n",
    "loss_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
